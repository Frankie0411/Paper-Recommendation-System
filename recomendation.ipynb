{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.datasets import KarateClub\n",
    "\n",
    "df=pd.read_json('53e9aa95b7602d970340bc5c_emb.json', lines='true',orient='records')\n",
    "df2=pd.read_json('53e9aa95b7602d970340bc5c.json', lines='true',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df[df['id'].isin(df2['id'])]\n",
    "del df\n",
    "df=df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist=[]\n",
    "for i in df.iterrows():\n",
    "    if i[1]['references']!=None:\n",
    "        for j in i[1]['references']:\n",
    "                edgelist.append((i[1]['id'],j))\n",
    "el=pd.DataFrame(edgelist,columns=['source','target'])\n",
    "el = el[el['target'].isin(df['id'])]\n",
    "G = nx.from_pandas_edgelist(el, 'source', 'target',create_using=nx.DiGraph())\n",
    "for i in df['id']:\n",
    "    G.add_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Labels: 2291\n",
      "Label Mapping: 2291\n",
      "(2291, 9)\n",
      "2291\n"
     ]
    }
   ],
   "source": [
    "papers=df['id'].tolist()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "labled_papers = label_encoder.fit_transform(papers)\n",
    "df['label']=labled_papers\n",
    "\n",
    "print(\"Numeric Labels:\", len(labled_papers))\n",
    "print(\"Label Mapping:\", len(list(label_encoder.classes_)))\n",
    "\n",
    "print(df.shape)\n",
    "print(G.number_of_nodes())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "el['sL']=label_encoder.transform(el['source'])\n",
    "el['tL']=label_encoder.transform(el['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list=[]\n",
    "edge_list.append(el['sL'].tolist())\n",
    "edge_list.append(el['tL'].tolist())\n",
    "edge_list=torch.tensor(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.datasets.karate.KarateClub"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = KarateClub()\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "data.x=torch.tensor(df['emb'].tolist())\n",
    "data.edge_index = torch.tensor(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, NamedTuple, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.utils import coalesce, scatter, softmax\n",
    "\n",
    "\n",
    "class UnpoolInfo(NamedTuple):\n",
    "    edge_index: Tensor\n",
    "    cluster: Tensor\n",
    "    batch: Tensor\n",
    "    new_edge_score: Tensor\n",
    "\n",
    "\n",
    "class EdgePooling(torch.nn.Module):\n",
    "    r\"\"\"The edge pooling operator from the `\"Towards Graph Pooling by Edge\n",
    "    Contraction\" <https://graphreason.github.io/papers/17.pdf>`__ and\n",
    "    `\"Edge Contraction Pooling for Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1905.10990>`__ papers.\n",
    "\n",
    "    In short, a score is computed for each edge.\n",
    "    Edges are contracted iteratively according to that score unless one of\n",
    "    their nodes has already been part of a contracted edge.\n",
    "\n",
    "    To duplicate the configuration from the `\"Towards Graph Pooling by Edge\n",
    "    Contraction\" <https://graphreason.github.io/papers/17.pdf>`__ paper, use\n",
    "    either :func:`EdgePooling.compute_edge_score_softmax`\n",
    "    or :func:`EdgePooling.compute_edge_score_tanh`, and set\n",
    "    :obj:`add_to_edge_score` to :obj:`0.0`.\n",
    "\n",
    "    To duplicate the configuration from the `\"Edge Contraction Pooling for\n",
    "    Graph Neural Networks\" <https://arxiv.org/abs/1905.10990>`__ paper,\n",
    "    set :obj:`dropout` to :obj:`0.2`.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        edge_score_method (callable, optional): The function to apply\n",
    "            to compute the edge score from raw edge scores. By default,\n",
    "            this is the softmax over all incoming edges for each node.\n",
    "            This function takes in a :obj:`raw_edge_score` tensor of shape\n",
    "            :obj:`[num_nodes]`, an :obj:`edge_index` tensor and the number of\n",
    "            nodes :obj:`num_nodes`, and produces a new tensor of the same size\n",
    "            as :obj:`raw_edge_score` describing normalized edge scores.\n",
    "            Included functions are\n",
    "            :func:`EdgePooling.compute_edge_score_softmax`,\n",
    "            :func:`EdgePooling.compute_edge_score_tanh`, and\n",
    "            :func:`EdgePooling.compute_edge_score_sigmoid`.\n",
    "            (default: :func:`EdgePooling.compute_edge_score_softmax`)\n",
    "        dropout (float, optional): The probability with\n",
    "            which to drop edge scores during training. (default: :obj:`0.0`)\n",
    "        add_to_edge_score (float, optional): A value to be added to each\n",
    "            computed edge score. Adding this greatly helps with unpooling\n",
    "            stability. (default: :obj:`0.5`)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        edge_score_method: Optional[Callable] = None,\n",
    "        dropout: Optional[float] = 0.0,\n",
    "        add_to_edge_score: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        if edge_score_method is None:\n",
    "            edge_score_method = self.compute_edge_score_softmax\n",
    "        self.compute_edge_score = edge_score_method\n",
    "        self.add_to_edge_score = add_to_edge_score\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin = torch.nn.Linear(2 * in_channels, 1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_edge_score_softmax(\n",
    "        raw_edge_score: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        num_nodes: int,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Normalizes edge scores via softmax application.\"\"\"\n",
    "        return softmax(raw_edge_score, edge_index[1], num_nodes=num_nodes)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_edge_score_tanh(\n",
    "        raw_edge_score: Tensor,\n",
    "        edge_index: Optional[Tensor] = None,\n",
    "        num_nodes: Optional[int] = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Normalizes edge scores via hyperbolic tangent application.\"\"\"\n",
    "        return torch.tanh(raw_edge_score)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_edge_score_sigmoid(\n",
    "        raw_edge_score: Tensor,\n",
    "        edge_index: Optional[Tensor] = None,\n",
    "        num_nodes: Optional[int] = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Normalizes edge scores via sigmoid application.\"\"\"\n",
    "        return torch.sigmoid(raw_edge_score)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        batch: Tensor,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, UnpoolInfo]:\n",
    "        r\"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The node features.\n",
    "            edge_index (torch.Tensor): The edge indices.\n",
    "            batch (torch.Tensor): The batch vector\n",
    "                :math:`\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N`, which assigns\n",
    "                each node to a specific example.\n",
    "\n",
    "        Return types:\n",
    "            * **x** *(torch.Tensor)* - The pooled node features.\n",
    "            * **edge_index** *(torch.Tensor)* - The coarsened edge indices.\n",
    "            * **batch** *(torch.Tensor)* - The coarsened batch vector.\n",
    "            * **unpool_info** *(UnpoolInfo)* - Information that is\n",
    "              consumed by :func:`EdgePooling.unpool` for unpooling.\n",
    "        \"\"\"\n",
    "        e = self.compute_edge_score(x, x,edge_index, x.size(0))\n",
    "        print(f'Edge Score: {e}')\n",
    "        print(f'Edge Score: {e.shape}')\n",
    "        e = F.dropout(e, p=self.dropout, training=self.training)\n",
    "        e = e + self.add_to_edge_score\n",
    "\n",
    "        x, edge_index, batch, unpool_info,cluster = self._merge_edges(\n",
    "            x, edge_index, batch, e)\n",
    "\n",
    "        return x, edge_index, batch, unpool_info,cluster\n",
    "\n",
    "    def _merge_edges(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        batch: Tensor,\n",
    "        edge_score: Tensor,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, UnpoolInfo]:\n",
    "\n",
    "        cluster = torch.empty_like(batch)\n",
    "        perm: List[int] = torch.argsort(edge_score, descending=True).tolist()\n",
    "\n",
    "        # Iterate through all edges, selecting it if it is not incident to\n",
    "        # another already chosen edge.\n",
    "        mask = torch.ones(x.size(0), dtype=torch.bool)\n",
    "\n",
    "        i = 0\n",
    "        new_edge_indices: List[int] = []\n",
    "        edge_index_cpu = edge_index.cpu()\n",
    "        for edge_idx in perm:\n",
    "            source = int(edge_index_cpu[0, edge_idx])\n",
    "            if not bool(mask[source]):\n",
    "                continue\n",
    "\n",
    "            target = int(edge_index_cpu[1, edge_idx])\n",
    "            if not bool(mask[target]):\n",
    "                continue\n",
    "\n",
    "            new_edge_indices.append(edge_idx)\n",
    "\n",
    "            cluster[source] = i\n",
    "            mask[source] = False\n",
    "\n",
    "            if source != target:\n",
    "                cluster[target] = i\n",
    "                mask[target] = False\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        # The remaining nodes are simply kept:\n",
    "        j = int(mask.sum())\n",
    "        cluster[mask] = torch.arange(i, i + j, device=x.device)\n",
    "        i += j\n",
    "\n",
    "        # We compute the new features as an addition of the old ones.\n",
    "        # new_x = scatter_mean(x, cluster, dim=0, dim_size=i)\n",
    "        new_x = scatter(x, cluster, dim=0, dim_size=cluster.max().item()+1, reduce='mean')\n",
    "        new_edge_score = edge_score[new_edge_indices]\n",
    "        if int(mask.sum()) > 0:\n",
    "            remaining_score = x.new_ones(\n",
    "                (new_x.size(0) - len(new_edge_indices), ))\n",
    "            new_edge_score = torch.cat([new_edge_score, remaining_score])\n",
    "        new_x = new_x * new_edge_score.view(-1, 1)\n",
    "\n",
    "        new_edge_index = coalesce(cluster[edge_index], num_nodes=new_x.size(0))\n",
    "        new_batch = x.new_empty(new_x.size(0), dtype=torch.long)\n",
    "        new_batch = new_batch.scatter_(0, cluster, batch)\n",
    "\n",
    "        unpool_info = UnpoolInfo(edge_index=edge_index, cluster=cluster,\n",
    "                                 batch=batch, new_edge_score=new_edge_score)\n",
    "\n",
    "        return new_x, new_edge_index, new_batch, unpool_info ,cluster\n",
    "\n",
    "    def unpool(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        unpool_info: UnpoolInfo,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        r\"\"\"Unpools a previous edge pooling step.\n",
    "\n",
    "        For unpooling, :obj:`x` should be of same shape as those produced by\n",
    "        this layer's :func:`forward` function. Then, it will produce an\n",
    "        unpooled :obj:`x` in addition to :obj:`edge_index` and :obj:`batch`.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The node features.\n",
    "            unpool_info (UnpoolInfo): Information that has been produced by\n",
    "                :func:`EdgePooling.forward`.\n",
    "\n",
    "        Return types:\n",
    "            * **x** *(torch.Tensor)* - The unpooled node features.\n",
    "            * **edge_index** *(torch.Tensor)* - The new edge indices.\n",
    "            * **batch** *(torch.Tensor)* - The new batch vector.\n",
    "        \"\"\"\n",
    "        new_x = x / unpool_info.new_edge_score.view(-1, 1)\n",
    "        new_x = new_x[unpool_info.cluster]\n",
    "        return new_x, unpool_info.edge_index, unpool_info.batch\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}({self.in_channels})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_edge_cosine_similarity(\n",
    "        source_node_features: Tensor,\n",
    "        target_node_features: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        num_nodes: Optional[int] = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Computes cosine similarity score for edges.\"\"\"\n",
    "        source_features = source_node_features[edge_index[0]]\n",
    "        target_features = target_node_features[edge_index[1]]\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine_sim = F.cosine_similarity(source_features, target_features, dim=1)\n",
    "        \n",
    "        return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class motif_ig(torch.nn.Module):\n",
    "    def __init__(self,num_features):\n",
    "        super(motif_ig, self).__init__()\n",
    "        self.pool_layer = EdgePooling(num_features, edge_score_method=compute_edge_cosine_similarity,add_to_edge_score=2,dropout=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x1 = self.pool_layer(x, edge_index, batch)\n",
    "        x2 = self.pool_layer(x1[0], x1[1], x1[2])\n",
    "        x3 = self.pool_layer(x2[0], x2[1], x2[2])\n",
    "        x4 = self.pool_layer(x3[0], x3[1], x3[2])\n",
    "        x5 = self.pool_layer(x4[0], x4[1], x4[2])\n",
    "        x6 = self.pool_layer(x5[0], x5[1], x5[2])\n",
    "        x7 = self.pool_layer(x6[0], x6[1], x6[2])\n",
    "        return x1, x2, x3 , x4, x5 , x6, x7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def cumilating(tensors): \n",
    "    clusters = {i: [i] for i in range(len(tensors[0]))}\n",
    "\n",
    "    def find(cluster, node):\n",
    "        while cluster[node] != node:\n",
    "            node = cluster[node]\n",
    "        return node\n",
    "\n",
    "    for tensor in tensors:\n",
    "        new_clusters = defaultdict(list)\n",
    "        for idx, value in enumerate(tensor):\n",
    "            new_clusters[int(value)].extend(clusters[idx])\n",
    "        \n",
    "        clusters = {key: list(set(value)) for key, value in new_clusters.items()}\n",
    "\n",
    "    final_clusters = list(clusters.values())\n",
    "\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Score: tensor([0.4157, 0.3419, 0.3957,  ..., 0.5924, 0.4100, 0.5104])\n",
      "Edge Score: torch.Size([9569])\n",
      "Edge Score: tensor([1.0000, 0.5383, 1.0000,  ..., 0.4684, 0.4208, 0.5011])\n",
      "Edge Score: torch.Size([6804])\n",
      "Edge Score: tensor([1.0000, 0.6903, 0.5981,  ..., 0.4990, 0.3884, 0.5011])\n",
      "Edge Score: torch.Size([6015])\n",
      "Edge Score: tensor([1.0000, 0.6950, 0.6609,  ..., 0.3542, 0.4993, 0.5242])\n",
      "Edge Score: torch.Size([5371])\n",
      "Edge Score: tensor([1.0000, 0.5729, 0.8202,  ..., 0.3943, 0.5033, 0.5242])\n",
      "Edge Score: torch.Size([4832])\n",
      "Edge Score: tensor([1.0000, 0.6093, 0.6777,  ..., 0.4132, 0.4823, 0.5242])\n",
      "Edge Score: torch.Size([4316])\n",
      "Edge Score: tensor([1.0000, 0.8382, 0.7440,  ..., 0.5436, 0.4620, 0.5242])\n",
      "Edge Score: torch.Size([3881])\n",
      "Edge Score: tensor([1.0000, 0.8010, 0.7982,  ..., 0.5724, 0.4127, 0.5242])\n",
      "Edge Score: torch.Size([3537])\n",
      "Edge Score: tensor([1.0000, 0.7660, 0.7401,  ..., 0.4290, 0.4154, 0.5242])\n",
      "Edge Score: torch.Size([3197])\n",
      "Edge Score: tensor([1.0000, 0.8085, 0.9019,  ..., 0.5675, 0.4680, 0.5242])\n",
      "Edge Score: torch.Size([2871])\n",
      "[[1027, 356, 748, 1006, 688, 1241, 698], [96, 993, 1698, 1192, 1100, 1293, 1395, 1749, 218, 1851, 764, 925], [1, 1285, 1669, 908, 1169, 1813, 21, 1815, 1175, 1308, 1313, 34, 1279, 934, 48, 1584, 818, 1202, 1850, 1468, 2109, 1470, 189, 1344, 1214, 963, 837, 1350, 965, 1612, 1613, 1359, 592, 1239, 1628, 1119, 1634, 359, 1768, 1514, 1002, 365, 1774, 1526, 1273, 1276, 511], [186, 1466], [1922, 1028, 1933, 144, 1301, 278, 2074, 1565, 1182, 2080, 1707, 1095, 1737, 1355, 1618, 1759, 1889, 1913, 1018, 2173, 2046], [1248, 1255, 722, 506, 1118], [1286, 777, 658, 1688, 1690, 1180, 803, 808, 940, 685, 1588, 1461, 311, 1593, 1339, 453, 99, 1892, 1898, 1131, 879, 245, 630, 2040, 383], [668, 2244, 2151, 376, 1082, 1756, 2264], [1833, 1828, 2245], [784, 2121], [2090, 798], [868, 102, 745, 489, 717, 1646, 178, 1785, 1498, 1469, 1662], [1925, 1926, 1927, 1931, 1710, 2066, 2069], [449, 1443, 1418, 139, 1610, 845, 1455, 567, 185, 1051, 1020, 797, 1438], [1783, 1903], [448, 897, 1254, 809, 170, 1421, 1586, 1944, 344, 1342], [309, 78], [896, 1185, 707, 1700, 615, 200, 1097, 1994, 1545, 843, 1386, 1326, 2128, 2225, 88, 1369, 2042, 2108], [2, 516, 69, 264, 1307], [2061, 2229], [41, 1327], [977, 2226, 341, 61], [2146, 1960, 1132, 2039, 1947, 2045, 2175], [393, 355, 246, 7], [2184, 2185, 2186, 2187], [767, 1039], [1923, 2181, 1932, 1426, 149, 281, 414, 2207, 1442, 1454, 1730, 839, 2131, 2004, 1245, 1373, 2143, 2153, 2161, 1527], [1190, 1374, 1958], [1251, 1412, 1417, 1482, 494, 1775, 86, 247], [1409, 643, 2182, 1031, 1928, 137, 1548, 398, 2064, 276, 1941, 791, 2073, 923, 1949, 1053, 1311, 1312, 417, 418, 676, 933, 422, 294, 678, 1577, 1450, 300, 2094, 1199, 51, 1977, 1978, 570, 1598, 1088, 961, 1347, 329, 1993, 1611, 2253, 973, 2257, 983, 2137, 2266, 2011, 2265, 1629, 2142, 95, 2017, 1890, 1508, 485, 2281, 1513, 2282, 2156, 1137, 1652, 2167, 888, 1657, 1277, 127], [1230], [752, 1532, 1622], [513, 1430], [858, 1786, 1287], [1156, 1674, 1549, 653, 1043, 533, 408, 1566, 1187, 684, 1203, 2101, 694, 565, 1980, 1218, 1868, 1231, 209, 1877, 1882, 1503, 1631, 493, 890, 1275, 1405], [715, 1541, 397], [1201, 2260], [772, 391], [618, 1130], [2114, 870], [2220, 2255], [1174, 742], [736, 813], [1578, 2259], [1792, 3, 1348, 1894, 1991, 1608, 1609, 1800, 1003, 335, 283, 1105, 1683, 342, 1178, 1115, 31], [2201, 1635, 1839, 1429, 1881], [68, 254], [1419, 2037, 2078], [401, 1173, 1816, 537, 1176, 1440, 680, 942, 1592, 834, 1221, 840, 75, 1228, 462, 222, 1504, 612, 228, 1382, 229], [1249, 231, 19, 1108, 1884], [1572, 1589], [2049, 1826, 2086, 2057, 2218, 1835, 2124, 2189, 1966, 1999, 1967, 2139, 2068, 2203, 2141, 1951], [1057, 1379, 1317, 340, 660, 1270, 823, 955, 252], [1449, 94, 1897], [58, 44, 1534, 119], [714, 1663], [697, 677, 1998, 1807], [327, 1227, 1138, 402, 503, 1338, 445], [971, 1491], [674, 898, 1030, 74, 2123, 906, 2095, 1968, 1649, 1364, 374, 1625, 1981], [512, 2060, 30, 1399], [336, 169, 580], [1976, 1445], [669, 431], [957, 1222], [1472, 1121, 550, 943, 1971, 1086], [1089, 330, 1803, 307, 886, 604], [1283, 259, 1539, 6, 1542, 1289, 1291, 523, 2063, 274, 1431, 409, 29, 802, 675, 1061, 295, 171, 1070, 1200, 180, 1463, 953, 1084, 1983, 575, 836, 332, 461, 208, 1104, 1232, 1361, 1107, 2261, 1750, 1495, 2136, 87, 605, 1377, 227, 616, 1257, 621, 2159, 629, 1531, 636, 1150], [611, 37, 1034, 275, 1950], [1761, 232, 428, 440, 249, 730], [1436, 173], [928, 832, 514, 549, 1767, 1771, 812, 1772, 1261, 1726, 659, 830], [689, 125], [1360, 1401, 762], [1537, 579, 1802, 1743, 1679, 1777, 691, 84, 1400, 505, 318], [289, 997, 520, 553, 1140, 24], [1195, 844], [23, 1509, 375], [1844, 2132], [1670], [1832, 348, 1023], [130, 1188], [2085, 1959, 2056, 2252, 2030, 1778, 2015, 351], [576, 162, 223, 583], [17, 970, 1651, 70], [1281, 67, 1477, 1413, 1163, 77, 110, 2169, 122, 1371, 2236, 1117], [1332, 551], [1831, 749, 1776, 1811, 1784, 1758], [642, 1123, 1864, 2059, 661, 1237, 2168, 220], [1444, 197, 2215], [922, 2262], [1530], [1538, 2020, 804, 1319, 167, 470, 919, 1021], [544, 2183, 202, 1066, 1324, 1867, 1484, 974, 1489, 1875], [226, 683, 639], [27, 1054], [2048, 2289], [1216, 390, 2023, 1459, 889, 1595], [1888, 1697, 1930, 1975], [651], [1393, 995, 1702], [388, 1540], [1244], [984, 2116, 255], [1267, 733], [1099, 2075], [969, 1354, 1876, 2007, 2110], [25, 573], [543], [705, 450], [769, 1258, 1235, 1848, 1370], [1441, 2034, 423], [9, 52], [1305, 469], [1962, 2083, 2084], [1446, 1068, 49, 1590, 1146, 1083, 2172, 1022], [1106, 1238], [65, 100], [866, 1331], [129, 1641], [1041, 2055], [711, 72, 8, 1336, 761, 1883, 988], [849, 455], [1952, 1101, 2270], [357, 917], [848, 741, 237], [2088, 370, 1556, 927, 63], [257, 1987, 260, 614, 233, 76, 656, 850, 1234, 381, 373, 253, 989, 447], [884, 1676], [1793, 515, 1155, 1420, 1295, 1040, 785, 918, 424, 46, 814, 303, 1974, 695, 1209, 1724, 1860, 457, 1225, 1872, 1873, 1499, 224, 2273, 1506, 490, 1645, 366], [2149, 400, 464, 950, 1016], [498, 1915, 382], [1024, 2246, 2031], [1272, 1414, 1607], [1456, 214], [826, 166], [2009, 1820], [1675, 786, 1172, 535, 921, 1693, 413, 1699, 298, 810, 1579, 1581, 174, 306, 436, 438, 1080, 825, 1741, 590, 723, 595, 1256, 1384, 750, 1905, 369, 754, 1013, 892], [256, 900, 267, 1037, 142, 910, 1424, 1551, 912, 789, 1302, 2071, 1437, 33, 546, 420, 164, 1064, 426, 1069, 430, 686, 1328, 177, 1713, 1473, 2241, 964, 325, 709, 1223, 593, 1619, 857, 92, 991, 482, 1250, 1124, 998, 1511, 488, 109, 1518, 1392, 753, 240, 627, 1522, 2036, 1142], [768, 1797, 1159, 647, 522, 399, 1557, 1942, 1943, 1953, 1316, 1574, 1063, 1582, 1071, 1205, 693, 310, 2230, 57, 442, 2235, 1732, 838, 199, 1478, 1481, 2249, 1870, 465, 860, 481, 121, 2019, 996, 1253, 873, 1899, 1516, 877, 881, 1653, 1654, 1912, 2041, 637, 766], [773, 775, 776, 1929, 1422, 1423, 1934, 1171, 1939, 790, 1562, 671, 800, 1183, 1186, 681, 42, 682, 941, 305, 948, 569, 1853, 1985, 322, 1603, 2243, 454, 1865, 1098, 1740, 978, 1879, 1752, 1755, 2268, 990, 738, 2021, 236, 1005, 1900, 108, 497, 1394, 2164, 1658], [1152, 1410, 899, 774, 519, 150, 666, 282, 157, 291, 806, 182, 439, 952, 1467, 1596, 703, 2239, 1615, 80, 851, 724, 725, 468, 607, 1375, 1505, 2275, 1636, 2277, 1764, 107, 875, 238, 760], [2050, 4, 903, 1799, 1032, 1801, 1165, 528, 788, 1045, 536, 1050, 159, 1439, 1570, 421, 1789, 1838, 1790, 304, 2097, 690, 563, 949, 2102, 1081, 60, 317, 1220, 1093, 1480, 591, 594, 2010, 2140, 477, 479, 2016, 2027, 1388, 1402, 765, 126], [5, 648, 1547, 1042, 280, 1945, 924, 416, 801, 1955, 1059, 1703, 1447, 179, 2117, 201, 713, 203, 1869, 847, 1490, 979, 1367, 1630, 1120, 1633, 354, 1125, 747, 492, 1014, 380], [1184, 640, 196, 473, 1677, 2062, 1709, 561, 1298, 1745, 1143, 633, 1335, 574, 319], [770, 258, 778, 652, 909, 525, 1425, 531, 1055, 32, 290, 292, 1458, 692, 820, 1206, 312, 1211, 1213, 1343, 1599, 1605, 841, 458, 588, 2002, 211, 853, 854, 85, 985, 347, 734, 352, 101, 1640, 1259, 235, 623, 1008, 1009, 1011, 115, 631], [1415, 1673, 524, 15, 1558, 920, 540, 1573, 944, 56, 975, 463, 597, 1111, 1496, 608, 2148, 1397, 1528, 1918], [265, 1035, 271, 1809, 1300, 22, 284, 1692, 285, 1824, 425, 1963, 2219, 557, 45, 1210, 1727, 2240, 1728, 831, 2115, 2118, 1616, 1617, 97, 1896, 1770], [1668, 1798, 1288, 1033, 18, 2070, 1432, 1840, 1457, 568, 1594, 62, 64, 720, 1110, 471, 2135, 1507, 1891, 2284, 2288, 882, 887, 1149], [143, 272, 883, 1494, 729, 1049, 1407], [2242, 40, 266, 2012, 664, 1681, 147, 1460, 1428, 2228, 1299, 472, 1916], [128, 1154, 2053, 135, 1160, 1162, 141, 2190, 781, 1296, 2065, 787, 2199, 151, 2204, 1310, 35, 1189, 2217, 1321, 1067, 2092, 2093, 1712, 2098, 1074, 1972, 1079, 1919, 2047, 444, 2237, 316, 2111, 1982, 1729, 194, 451, 1862, 1735, 1863, 589, 1358, 1485, 1746, 596, 346, 1242, 867, 1515, 620, 1517, 624, 2035, 628, 1396, 1017, 507, 1535], [2208, 1126, 1158, 1638, 1065, 1196, 1550, 2222, 1329, 82, 855, 251, 956, 541, 1502], [1056, 992, 230, 1128, 2126, 1911, 880, 1559, 1852, 1406, 1087], [1408, 2113, 1954, 387, 999, 1834, 2171, 1970, 219, 412], [1280, 1856, 1910, 1721, 859], [1637, 2152, 1290, 1004, 782, 2192, 499], [2176, 1026, 1381, 2214, 901, 1544, 396, 1487, 372, 250, 1147], [389, 1766, 1320, 649, 939, 1773, 1904, 532, 1340], [577, 874, 1739, 1642, 2256, 145, 945, 560, 2096, 817, 1754, 1626, 795, 446], [193, 1866, 496, 1714, 155], [188, 571, 1036, 1181], [610, 195, 518, 2122, 1323, 1648], [704, 650, 2000, 287], [2112, 864, 2051, 1829, 1157, 2022, 2248, 2029, 2174], [1893, 165, 168, 559, 1524], [613, 1102, 1168, 562, 819], [932, 861, 1260, 367], [2043, 1262], [1352, 805, 751], [1722, 1725], [951, 1639], [120], [225, 2147, 261, 1744, 1010, 1076, 2104, 89], [1451, 556], [154], [1632, 1217, 2188, 958, 702], [1568, 1989, 1680, 1817, 1278], [634, 1597, 286], [872, 1995], [1380, 38], [581, 972, 1488, 242, 213, 501], [1264, 131, 1580, 2179], [931, 452, 1198], [1177, 835], [1252, 2276, 405, 1109, 124], [112, 1038], [1753, 1479], [28, 1229], [2145, 395, 1996, 2193, 1782, 1818], [384, 98, 1096, 521, 2058, 1161, 1356, 796, 926], [2024], [337, 811, 1233, 1814], [2234, 371, 1887], [1372, 14], [905, 2290, 1269], [368, 891], [16, 296], [1650, 1325, 1510, 1247], [288, 491, 1268], [433, 1127], [487, 2279], [980], [1808, 1554, 269], [184, 1129], [1464, 1689], [2210, 1742], [2008, 735, 2087], [0, 12], [1144, 268], [878, 1015], [1656, 701, 1655], [419, 1411], [1521, 869, 822], [625, 486], [885], [712, 508], [434, 206], [1145, 394], [1274, 1334], [721, 876], [893, 1062], [946, 133], [1546, 204, 783], [904], [1624, 466, 156], [1858, 2163, 2212, 2138], [1435, 1191], [2091, 315, 1874, 2134, 986, 123, 478], [415, 913, 1462, 1135], [673, 1571, 2247, 73, 1961, 339, 1748, 1046, 343], [36, 679], [960, 1543], [584, 1134], [1921, 1330], [1937, 2165], [1560, 83, 1845], [1433, 852, 1151], [429, 710], [1920, 1924, 20, 670], [338, 740], [1092, 1078], [1297, 1390], [1001, 1964, 1823, 1847], [435, 967], [1561, 90], [865, 2081], [91, 2238], [1292, 1694], [833, 11], [665, 1621, 1751], [824], [403, 111], [345, 323, 221, 1747], [1265, 1403], [609, 1525], [1553], [1072, 827, 667], [10], [13], [26], [39], [43], [47], [50], [53], [54], [55], [59], [66], [71], [79], [81], [93], [103], [104], [105], [106], [113], [114], [116], [117], [118], [132], [134], [136], [138], [140], [146], [148], [152], [153], [158], [160], [161], [163], [172], [175], [176], [181], [183], [187], [190], [191], [192], [198], [205], [207], [210], [212], [215], [216], [217], [234], [239], [241], [243], [244], [248], [262], [263], [270], [273], [277], [279], [293], [297], [299], [301], [302], [308], [313], [314], [320], [321], [324], [326], [328], [331], [333], [334], [349], [350], [353], [358], [360], [361], [362], [363], [364], [377], [378], [379], [385], [386], [392], [404], [406], [407], [410], [411], [427], [432], [437], [441], [443], [456], [459], [460], [467], [474], [475], [476], [480], [483], [484], [495], [500], [502], [504], [509], [510], [517], [526], [527], [529], [530], [534], [538], [539], [542], [545], [547], [548], [552], [554], [555], [558], [564], [566], [572], [578], [582], [585], [586], [587], [598], [599], [600], [601], [602], [603], [606], [617], [619], [622], [626], [632], [635], [638], [641], [644], [645], [646], [654], [655], [657], [662], [663], [672], [687], [696], [699], [700], [706], [708], [716], [718], [719], [726], [727], [728], [731], [732], [737], [739], [743], [744], [746], [755], [756], [757], [758], [759], [763], [771], [779], [780], [792], [793], [794], [799], [807], [815], [816], [821], [828], [829], [842], [846], [856], [862], [863], [871], [894], [895], [902], [907], [911], [914], [915], [916], [929], [930], [935], [936], [937], [938], [947], [954], [959], [962], [966], [968], [976], [981], [982], [987], [994], [1000], [1007], [1012], [1019], [1025], [1029], [1044], [1047], [1048], [1052], [1058], [1060], [1073], [1075], [1077], [1085], [1090], [1091], [1094], [1103], [1112], [1113], [1114], [1116], [1122], [1133], [1136], [1139], [1141], [1148], [1153], [1164], [1166], [1167], [1170], [1179], [1193], [1194], [1197], [1204], [1207], [1208], [1212], [1215], [1219], [1224], [1226], [1236], [1240], [1243], [1246], [1263], [1266], [1271], [1282], [1284], [1294], [1303], [1304], [1306], [1309], [1314], [1315], [1318], [1322], [1333], [1337], [1341], [1345], [1346], [1349], [1351], [1353], [1357], [1362], [1363], [1365], [1366], [1368], [1376], [1378], [1383], [1385], [1387], [1389], [1391], [1398], [1404], [1416], [1427], [1434], [1448], [1452], [1453], [1465], [1471], [1474], [1475], [1476], [1483], [1486], [1492], [1493], [1497], [1500], [1501], [1512], [1519], [1520], [1523], [1529], [1533], [1536], [1552], [1555], [1563], [1564], [1567], [1569], [1575], [1576], [1583], [1585], [1587], [1591], [1600], [1601], [1602], [1604], [1606], [1614], [1620], [1623], [1627], [1643], [1644], [1647], [1659], [1660], [1661], [1664], [1665], [1666], [1667], [1671], [1672], [1678], [1682], [1684], [1685], [1686], [1687], [1691], [1695], [1696], [1701], [1704], [1705], [1706], [1708], [1711], [1715], [1716], [1717], [1718], [1719], [1720], [1723], [1731], [1733], [1734], [1736], [1738], [1757], [1760], [1762], [1763], [1765], [1769], [1779], [1780], [1781], [1787], [1788], [1791], [1794], [1795], [1796], [1804], [1805], [1806], [1810], [1812], [1819], [1821], [1822], [1825], [1827], [1830], [1836], [1837], [1841], [1842], [1843], [1846], [1849], [1854], [1855], [1857], [1859], [1861], [1871], [1878], [1880], [1885], [1886], [1895], [1901], [1902], [1906], [1907], [1908], [1909], [1914], [1917], [1935], [1936], [1938], [1940], [1946], [1948], [1956], [1957], [1965], [1969], [1973], [1979], [1984], [1986], [1988], [1990], [1992], [1997], [2001], [2003], [2005], [2006], [2013], [2014], [2018], [2025], [2026], [2028], [2032], [2033], [2038], [2044], [2052], [2054], [2067], [2072], [2076], [2077], [2079], [2082], [2089], [2099], [2100], [2103], [2105], [2106], [2107], [2119], [2120], [2125], [2127], [2129], [2130], [2133], [2144], [2150], [2154], [2155], [2157], [2158], [2160], [2162], [2166], [2170], [2177], [2178], [2180], [2191], [2194], [2195], [2196], [2197], [2198], [2200], [2202], [2205], [2206], [2209], [2211], [2213], [2216], [2221], [2223], [2224], [2227], [2231], [2232], [2233], [2250], [2251], [2254], [2258], [2263], [2267], [2269], [2271], [2272], [2274], [2278], [2280], [2283], [2285], [2286], [2287]]\n"
     ]
    }
   ],
   "source": [
    "l=[0]*data.num_nodes\n",
    "batch = torch.tensor(l, dtype=torch.long)\n",
    "\n",
    "motifs=motif_ig(data.num_features)\n",
    "x=motifs(data.x, data.edge_index, batch)\n",
    "\n",
    "tensors = [i[3][1] for i in x]\n",
    "i=cumilating(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "G=nx.from_pandas_edgelist(el, 'sL', 'tL',create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {}\n",
    "color_index = 0\n",
    "for pair in i:\n",
    "    for node in pair:\n",
    "        color_map[node] = color_index\n",
    "    color_index += 1\n",
    "node_colors = [color_map[node] for node in G.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "lol=df['id'].tolist()\n",
    "a=random.choice(lol)\n",
    "a=df[df['id']==a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_embeddings=x[len(x)-1][0]\n",
    "a_emb=torch.tensor(a['emb'].values[0])   \n",
    "a_emb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "motifs_embeddings=x[len(x)-1][0]\n",
    "a_emb=torch.tensor(a['emb'].values[0])   \n",
    "a_emb.shape\n",
    "\n",
    "def cosine_similarity(emb1, emb2):\n",
    "    emb1_tensor = torch.tensor(emb1)\n",
    "    emb2_tensor = torch.tensor(emb2)\n",
    "    return F.cosine_similarity(emb1_tensor, emb2_tensor, dim=0).float()\n",
    "\n",
    "ranks = {}\n",
    "for i in range(motifs_embeddings.shape[0]):\n",
    "    ranks[i] = cosine_similarity(motifs_embeddings[i], a_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{28: tensor(0.8351), 103: tensor(0.8022), 140: tensor(0.7994), 111: tensor(0.7960), 148: tensor(0.7918), 109: tensor(0.7849), 68: tensor(0.7831), 142: tensor(0.7805), 116: tensor(0.7803), 147: tensor(0.7777), 81: tensor(0.7775), 1: tensor(0.7735), 145: tensor(0.7678), 141: tensor(0.7662), 144: tensor(0.7656), 508: tensor(0.7652), 204: tensor(0.7593), 406: tensor(0.7592), 38: tensor(0.7557), 121: tensor(0.7551), 16: tensor(0.7524), 571: tensor(0.7511), 40: tensor(0.7491), 117: tensor(0.7491), 26: tensor(0.7477), 47: tensor(0.7467), 157: tensor(0.7453), 2: tensor(0.7445), 172: tensor(0.7437), 405: tensor(0.7435), 65: tensor(0.7408), 48: tensor(0.7393), 128: tensor(0.7372), 50: tensor(0.7353), 815: tensor(0.7351), 151: tensor(0.7349), 462: tensor(0.7346), 64: tensor(0.7338), 201: tensor(0.7333), 86: tensor(0.7333), 152: tensor(0.7331), 3: tensor(0.7327), 554: tensor(0.7308), 216: tensor(0.7307), 781: tensor(0.7302), 170: tensor(0.7300), 149: tensor(0.7278), 601: tensor(0.7267), 156: tensor(0.7252), 146: tensor(0.7248), 223: tensor(0.7244), 153: tensor(0.7240), 66: tensor(0.7235), 552: tensor(0.7231), 118: tensor(0.7229), 69: tensor(0.7225), 180: tensor(0.7224), 143: tensor(0.7223), 63: tensor(0.7211), 137: tensor(0.7209), 150: tensor(0.7168), 752: tensor(0.7168), 92: tensor(0.7166), 246: tensor(0.7160), 113: tensor(0.7154), 83: tensor(0.7153), 39: tensor(0.7152), 403: tensor(0.7139), 780: tensor(0.7136), 30: tensor(0.7134), 53: tensor(0.7116), 100: tensor(0.7102), 224: tensor(0.7076), 434: tensor(0.7073), 165: tensor(0.7066), 104: tensor(0.7058), 336: tensor(0.7051), 46: tensor(0.7044), 74: tensor(0.7041), 21: tensor(0.7037), 122: tensor(0.7032), 579: tensor(0.7022), 557: tensor(0.7016), 93: tensor(0.7009), 253: tensor(0.6997), 724: tensor(0.6985), 811: tensor(0.6983), 126: tensor(0.6979), 227: tensor(0.6977), 398: tensor(0.6976), 396: tensor(0.6976), 570: tensor(0.6959), 23: tensor(0.6946), 36: tensor(0.6944), 248: tensor(0.6939), 711: tensor(0.6938), 220: tensor(0.6935), 701: tensor(0.6920), 0: tensor(0.6914), 18: tensor(0.6911), 318: tensor(0.6908), 200: tensor(0.6907), 134: tensor(0.6900), 35: tensor(0.6895), 645: tensor(0.6885), 213: tensor(0.6879), 214: tensor(0.6865), 43: tensor(0.6862), 332: tensor(0.6859), 432: tensor(0.6857), 71: tensor(0.6845), 62: tensor(0.6840), 673: tensor(0.6835), 34: tensor(0.6830), 727: tensor(0.6830), 33: tensor(0.6827), 55: tensor(0.6826), 32: tensor(0.6823), 27: tensor(0.6821), 8: tensor(0.6815), 460: tensor(0.6811), 784: tensor(0.6787), 187: tensor(0.6784), 88: tensor(0.6783), 335: tensor(0.6780), 124: tensor(0.6776), 618: tensor(0.6773), 790: tensor(0.6772), 654: tensor(0.6763), 154: tensor(0.6757), 369: tensor(0.6747), 787: tensor(0.6734), 337: tensor(0.6732), 166: tensor(0.6732), 79: tensor(0.6729), 228: tensor(0.6714), 569: tensor(0.6695), 85: tensor(0.6694), 532: tensor(0.6690), 230: tensor(0.6686), 397: tensor(0.6684), 643: tensor(0.6684), 606: tensor(0.6682), 108: tensor(0.6681), 78: tensor(0.6674), 219: tensor(0.6673), 788: tensor(0.6670), 459: tensor(0.6664), 236: tensor(0.6663), 465: tensor(0.6660), 173: tensor(0.6656), 217: tensor(0.6656), 168: tensor(0.6651), 240: tensor(0.6636), 239: tensor(0.6634), 593: tensor(0.6630), 161: tensor(0.6630), 766: tensor(0.6627), 779: tensor(0.6627), 80: tensor(0.6615), 327: tensor(0.6613), 212: tensor(0.6612), 105: tensor(0.6599), 392: tensor(0.6599), 192: tensor(0.6593), 639: tensor(0.6590), 229: tensor(0.6580), 372: tensor(0.6576), 250: tensor(0.6570), 821: tensor(0.6567), 347: tensor(0.6564), 160: tensor(0.6562), 20: tensor(0.6561), 235: tensor(0.6557), 44: tensor(0.6553), 203: tensor(0.6552), 183: tensor(0.6548), 825: tensor(0.6541), 198: tensor(0.6538), 389: tensor(0.6520), 707: tensor(0.6515), 688: tensor(0.6511), 169: tensor(0.6507), 378: tensor(0.6506), 773: tensor(0.6498), 617: tensor(0.6492), 319: tensor(0.6484), 823: tensor(0.6484), 17: tensor(0.6482), 314: tensor(0.6476), 620: tensor(0.6466), 555: tensor(0.6463), 542: tensor(0.6457), 478: tensor(0.6453), 820: tensor(0.6450), 519: tensor(0.6443), 4: tensor(0.6437), 799: tensor(0.6430), 310: tensor(0.6426), 189: tensor(0.6414), 467: tensor(0.6396), 492: tensor(0.6395), 98: tensor(0.6381), 196: tensor(0.6379), 197: tensor(0.6377), 323: tensor(0.6376), 594: tensor(0.6372), 184: tensor(0.6372), 115: tensor(0.6371), 613: tensor(0.6368), 556: tensor(0.6367), 72: tensor(0.6366), 564: tensor(0.6362), 182: tensor(0.6352), 60: tensor(0.6352), 504: tensor(0.6350), 822: tensor(0.6347), 77: tensor(0.6346), 96: tensor(0.6340), 278: tensor(0.6335), 749: tensor(0.6329), 222: tensor(0.6324), 451: tensor(0.6312), 211: tensor(0.6309), 70: tensor(0.6308), 252: tensor(0.6307), 685: tensor(0.6307), 368: tensor(0.6300), 827: tensor(0.6290), 31: tensor(0.6288), 95: tensor(0.6283), 207: tensor(0.6275), 191: tensor(0.6269), 176: tensor(0.6263), 10: tensor(0.6261), 193: tensor(0.6261), 112: tensor(0.6255), 7: tensor(0.6253), 41: tensor(0.6252), 354: tensor(0.6245), 308: tensor(0.6245), 247: tensor(0.6244), 452: tensor(0.6241), 689: tensor(0.6240), 267: tensor(0.6235), 512: tensor(0.6234), 205: tensor(0.6223), 106: tensor(0.6223), 9: tensor(0.6218), 383: tensor(0.6215), 171: tensor(0.6215), 73: tensor(0.6215), 155: tensor(0.6212), 502: tensor(0.6210), 159: tensor(0.6201), 199: tensor(0.6197), 798: tensor(0.6190), 101: tensor(0.6179), 521: tensor(0.6169), 808: tensor(0.6169), 562: tensor(0.6168), 6: tensor(0.6167), 225: tensor(0.6166), 341: tensor(0.6163), 325: tensor(0.6162), 726: tensor(0.6155), 761: tensor(0.6155), 243: tensor(0.6154), 174: tensor(0.6150), 19: tensor(0.6143), 89: tensor(0.6141), 624: tensor(0.6134), 750: tensor(0.6129), 409: tensor(0.6123), 655: tensor(0.6118), 708: tensor(0.6118), 807: tensor(0.6113), 125: tensor(0.6112), 107: tensor(0.6093), 484: tensor(0.6092), 87: tensor(0.6087), 417: tensor(0.6078), 256: tensor(0.6074), 42: tensor(0.6073), 447: tensor(0.6065), 566: tensor(0.6062), 202: tensor(0.6058), 768: tensor(0.6048), 206: tensor(0.6048), 362: tensor(0.6042), 357: tensor(0.6040), 546: tensor(0.6038), 488: tensor(0.6035), 257: tensor(0.6033), 659: tensor(0.6027), 758: tensor(0.6026), 767: tensor(0.6026), 304: tensor(0.6023), 262: tensor(0.6022), 515: tensor(0.6018), 797: tensor(0.6015), 317: tensor(0.6013), 824: tensor(0.6011), 210: tensor(0.6011), 565: tensor(0.6009), 346: tensor(0.6005), 234: tensor(0.6002), 14: tensor(0.6002), 61: tensor(0.5999), 526: tensor(0.5997), 363: tensor(0.5991), 120: tensor(0.5991), 548: tensor(0.5987), 249: tensor(0.5981), 736: tensor(0.5980), 272: tensor(0.5979), 777: tensor(0.5976), 485: tensor(0.5975), 242: tensor(0.5967), 759: tensor(0.5965), 448: tensor(0.5956), 608: tensor(0.5955), 785: tensor(0.5945), 602: tensor(0.5943), 511: tensor(0.5940), 29: tensor(0.5938), 380: tensor(0.5935), 801: tensor(0.5931), 672: tensor(0.5930), 725: tensor(0.5930), 350: tensor(0.5928), 611: tensor(0.5925), 490: tensor(0.5920), 763: tensor(0.5918), 395: tensor(0.5913), 324: tensor(0.5912), 543: tensor(0.5912), 818: tensor(0.5911), 453: tensor(0.5910), 776: tensor(0.5907), 311: tensor(0.5899), 194: tensor(0.5898), 119: tensor(0.5897), 516: tensor(0.5896), 381: tensor(0.5894), 482: tensor(0.5889), 697: tensor(0.5885), 178: tensor(0.5880), 281: tensor(0.5879), 415: tensor(0.5877), 313: tensor(0.5873), 94: tensor(0.5871), 559: tensor(0.5871), 693: tensor(0.5864), 671: tensor(0.5861), 245: tensor(0.5858), 139: tensor(0.5857), 738: tensor(0.5857), 647: tensor(0.5856), 466: tensor(0.5853), 320: tensor(0.5847), 769: tensor(0.5844), 291: tensor(0.5843), 536: tensor(0.5840), 667: tensor(0.5840), 728: tensor(0.5840), 375: tensor(0.5834), 58: tensor(0.5829), 273: tensor(0.5828), 411: tensor(0.5827), 244: tensor(0.5821), 572: tensor(0.5821), 695: tensor(0.5811), 179: tensor(0.5811), 495: tensor(0.5811), 741: tensor(0.5810), 209: tensor(0.5809), 91: tensor(0.5791), 514: tensor(0.5789), 402: tensor(0.5784), 57: tensor(0.5771), 54: tensor(0.5769), 723: tensor(0.5767), 67: tensor(0.5765), 401: tensor(0.5746), 295: tensor(0.5745), 541: tensor(0.5734), 714: tensor(0.5730), 804: tensor(0.5729), 158: tensor(0.5728), 629: tensor(0.5718), 527: tensor(0.5716), 45: tensor(0.5712), 567: tensor(0.5711), 679: tensor(0.5709), 238: tensor(0.5701), 299: tensor(0.5700), 429: tensor(0.5699), 739: tensor(0.5695), 491: tensor(0.5695), 474: tensor(0.5693), 367: tensor(0.5690), 382: tensor(0.5682), 12: tensor(0.5677), 632: tensor(0.5676), 681: tensor(0.5673), 135: tensor(0.5673), 190: tensor(0.5659), 226: tensor(0.5653), 360: tensor(0.5652), 800: tensor(0.5643), 709: tensor(0.5641), 442: tensor(0.5635), 530: tensor(0.5633), 195: tensor(0.5630), 486: tensor(0.5621), 301: tensor(0.5616), 487: tensor(0.5615), 167: tensor(0.5612), 605: tensor(0.5612), 819: tensor(0.5611), 185: tensor(0.5608), 163: tensor(0.5608), 269: tensor(0.5603), 600: tensor(0.5602), 599: tensor(0.5601), 251: tensor(0.5594), 394: tensor(0.5586), 370: tensor(0.5580), 778: tensor(0.5572), 631: tensor(0.5572), 25: tensor(0.5571), 609: tensor(0.5570), 461: tensor(0.5564), 175: tensor(0.5563), 237: tensor(0.5557), 284: tensor(0.5557), 574: tensor(0.5554), 49: tensor(0.5546), 377: tensor(0.5543), 568: tensor(0.5542), 315: tensor(0.5538), 84: tensor(0.5532), 545: tensor(0.5529), 791: tensor(0.5527), 221: tensor(0.5523), 102: tensor(0.5522), 704: tensor(0.5519), 271: tensor(0.5516), 523: tensor(0.5504), 285: tensor(0.5501), 388: tensor(0.5500), 757: tensor(0.5497), 627: tensor(0.5491), 742: tensor(0.5488), 473: tensor(0.5478), 640: tensor(0.5472), 740: tensor(0.5468), 435: tensor(0.5468), 276: tensor(0.5464), 471: tensor(0.5461), 441: tensor(0.5459), 586: tensor(0.5449), 737: tensor(0.5445), 400: tensor(0.5438), 186: tensor(0.5432), 127: tensor(0.5431), 164: tensor(0.5428), 75: tensor(0.5428), 746: tensor(0.5427), 786: tensor(0.5422), 721: tensor(0.5420), 814: tensor(0.5415), 330: tensor(0.5414), 215: tensor(0.5405), 683: tensor(0.5398), 458: tensor(0.5390), 440: tensor(0.5378), 710: tensor(0.5377), 24: tensor(0.5373), 533: tensor(0.5371), 376: tensor(0.5366), 660: tensor(0.5361), 623: tensor(0.5339), 589: tensor(0.5339), 59: tensor(0.5334), 720: tensor(0.5331), 410: tensor(0.5327), 418: tensor(0.5326), 261: tensor(0.5324), 619: tensor(0.5322), 136: tensor(0.5321), 413: tensor(0.5319), 433: tensor(0.5317), 241: tensor(0.5316), 5: tensor(0.5310), 662: tensor(0.5304), 765: tensor(0.5303), 509: tensor(0.5292), 626: tensor(0.5291), 539: tensor(0.5288), 610: tensor(0.5281), 702: tensor(0.5281), 792: tensor(0.5280), 598: tensor(0.5279), 771: tensor(0.5272), 760: tensor(0.5268), 580: tensor(0.5263), 15: tensor(0.5262), 540: tensor(0.5261), 11: tensor(0.5258), 813: tensor(0.5255), 351: tensor(0.5253), 123: tensor(0.5250), 503: tensor(0.5244), 692: tensor(0.5241), 130: tensor(0.5239), 687: tensor(0.5231), 794: tensor(0.5228), 349: tensor(0.5228), 477: tensor(0.5226), 578: tensor(0.5211), 255: tensor(0.5205), 644: tensor(0.5198), 188: tensor(0.5194), 138: tensor(0.5191), 475: tensor(0.5191), 133: tensor(0.5189), 162: tensor(0.5178), 588: tensor(0.5172), 456: tensor(0.5170), 544: tensor(0.5162), 481: tensor(0.5158), 675: tensor(0.5158), 132: tensor(0.5157), 665: tensor(0.5151), 431: tensor(0.5147), 404: tensor(0.5146), 661: tensor(0.5146), 498: tensor(0.5143), 444: tensor(0.5143), 535: tensor(0.5139), 732: tensor(0.5130), 289: tensor(0.5126), 501: tensor(0.5125), 648: tensor(0.5118), 457: tensor(0.5117), 208: tensor(0.5114), 493: tensor(0.5110), 390: tensor(0.5108), 436: tensor(0.5104), 131: tensor(0.5103), 691: tensor(0.5102), 525: tensor(0.5094), 312: tensor(0.5091), 537: tensor(0.5086), 551: tensor(0.5085), 774: tensor(0.5083), 812: tensor(0.5083), 520: tensor(0.5077), 751: tensor(0.5075), 677: tensor(0.5073), 280: tensor(0.5071), 343: tensor(0.5069), 616: tensor(0.5063), 231: tensor(0.5044), 419: tensor(0.5044), 744: tensor(0.5038), 296: tensor(0.5028), 743: tensor(0.5026), 690: tensor(0.5023), 379: tensor(0.5022), 703: tensor(0.5016), 806: tensor(0.5015), 345: tensor(0.5014), 528: tensor(0.5013), 573: tensor(0.5012), 391: tensor(0.5011), 232: tensor(0.5011), 428: tensor(0.5007), 817: tensor(0.5007), 416: tensor(0.5006), 719: tensor(0.4999), 658: tensor(0.4998), 497: tensor(0.4995), 22: tensor(0.4992), 359: tensor(0.4987), 734: tensor(0.4987), 561: tensor(0.4983), 282: tensor(0.4971), 782: tensor(0.4968), 352: tensor(0.4956), 274: tensor(0.4954), 630: tensor(0.4953), 649: tensor(0.4953), 427: tensor(0.4942), 789: tensor(0.4942), 663: tensor(0.4941), 700: tensor(0.4941), 717: tensor(0.4939), 756: tensor(0.4932), 56: tensor(0.4928), 329: tensor(0.4911), 496: tensor(0.4907), 538: tensor(0.4904), 517: tensor(0.4904), 684: tensor(0.4892), 233: tensor(0.4887), 302: tensor(0.4880), 783: tensor(0.4874), 637: tensor(0.4870), 443: tensor(0.4870), 129: tensor(0.4863), 13: tensor(0.4856), 218: tensor(0.4852), 293: tensor(0.4851), 698: tensor(0.4850), 340: tensor(0.4841), 364: tensor(0.4841), 114: tensor(0.4832), 408: tensor(0.4832), 414: tensor(0.4831), 181: tensor(0.4829), 297: tensor(0.4828), 275: tensor(0.4809), 634: tensor(0.4809), 729: tensor(0.4804), 531: tensor(0.4800), 676: tensor(0.4795), 328: tensor(0.4794), 470: tensor(0.4792), 678: tensor(0.4789), 686: tensor(0.4787), 421: tensor(0.4786), 51: tensor(0.4782), 506: tensor(0.4776), 582: tensor(0.4755), 587: tensor(0.4748), 604: tensor(0.4746), 344: tensor(0.4744), 576: tensor(0.4733), 735: tensor(0.4725), 584: tensor(0.4720), 694: tensor(0.4711), 614: tensor(0.4709), 563: tensor(0.4709), 322: tensor(0.4698), 499: tensor(0.4696), 426: tensor(0.4692), 342: tensor(0.4669), 90: tensor(0.4668), 636: tensor(0.4662), 522: tensor(0.4657), 177: tensor(0.4629), 374: tensor(0.4617), 656: tensor(0.4610), 764: tensor(0.4603), 680: tensor(0.4598), 468: tensor(0.4586), 762: tensor(0.4584), 258: tensor(0.4581), 716: tensor(0.4580), 407: tensor(0.4573), 713: tensor(0.4573), 772: tensor(0.4570), 288: tensor(0.4566), 733: tensor(0.4562), 480: tensor(0.4557), 454: tensor(0.4550), 384: tensor(0.4549), 358: tensor(0.4546), 607: tensor(0.4545), 591: tensor(0.4540), 439: tensor(0.4535), 386: tensor(0.4529), 770: tensor(0.4520), 505: tensor(0.4515), 500: tensor(0.4506), 718: tensor(0.4503), 642: tensor(0.4496), 534: tensor(0.4496), 333: tensor(0.4487), 424: tensor(0.4471), 356: tensor(0.4440), 669: tensor(0.4436), 82: tensor(0.4435), 420: tensor(0.4434), 385: tensor(0.4433), 361: tensor(0.4416), 793: tensor(0.4387), 99: tensor(0.4375), 476: tensor(0.4371), 399: tensor(0.4364), 795: tensor(0.4361), 270: tensor(0.4358), 279: tensor(0.4350), 796: tensor(0.4348), 592: tensor(0.4344), 585: tensor(0.4340), 621: tensor(0.4330), 809: tensor(0.4312), 348: tensor(0.4307), 581: tensor(0.4307), 298: tensor(0.4266), 547: tensor(0.4234), 549: tensor(0.4227), 595: tensor(0.4214), 464: tensor(0.4206), 596: tensor(0.4206), 650: tensor(0.4196), 638: tensor(0.4189), 286: tensor(0.4181), 52: tensor(0.4180), 518: tensor(0.4178), 577: tensor(0.4171), 745: tensor(0.4169), 334: tensor(0.4158), 263: tensor(0.4153), 463: tensor(0.4139), 615: tensor(0.4135), 722: tensor(0.4114), 524: tensor(0.4107), 110: tensor(0.4103), 277: tensor(0.4100), 730: tensor(0.4090), 423: tensor(0.4087), 682: tensor(0.4085), 553: tensor(0.4083), 355: tensor(0.4079), 670: tensor(0.4069), 326: tensor(0.4069), 254: tensor(0.4069), 731: tensor(0.4065), 264: tensor(0.4046), 268: tensor(0.4039), 558: tensor(0.4025), 666: tensor(0.4019), 339: tensor(0.4006), 366: tensor(0.4005), 97: tensor(0.3995), 393: tensor(0.3979), 674: tensor(0.3973), 259: tensor(0.3964), 646: tensor(0.3957), 641: tensor(0.3952), 706: tensor(0.3952), 437: tensor(0.3943), 412: tensor(0.3937), 331: tensor(0.3926), 754: tensor(0.3924), 266: tensor(0.3922), 715: tensor(0.3918), 309: tensor(0.3906), 575: tensor(0.3905), 321: tensor(0.3892), 387: tensor(0.3889), 445: tensor(0.3869), 803: tensor(0.3868), 816: tensor(0.3855), 652: tensor(0.3845), 603: tensor(0.3842), 657: tensor(0.3810), 316: tensor(0.3800), 455: tensor(0.3794), 513: tensor(0.3788), 633: tensor(0.3777), 489: tensor(0.3762), 260: tensor(0.3761), 550: tensor(0.3746), 446: tensor(0.3744), 303: tensor(0.3733), 338: tensor(0.3679), 747: tensor(0.3660), 699: tensor(0.3652), 635: tensor(0.3629), 365: tensor(0.3622), 292: tensor(0.3599), 76: tensor(0.3583), 696: tensor(0.3571), 755: tensor(0.3541), 450: tensor(0.3540), 483: tensor(0.3539), 290: tensor(0.3538), 529: tensor(0.3536), 753: tensor(0.3531), 805: tensor(0.3524), 802: tensor(0.3523), 425: tensor(0.3521), 265: tensor(0.3505), 560: tensor(0.3487), 510: tensor(0.3484), 307: tensor(0.3479), 625: tensor(0.3455), 438: tensor(0.3448), 371: tensor(0.3443), 449: tensor(0.3404), 353: tensor(0.3399), 653: tensor(0.3385), 705: tensor(0.3357), 300: tensor(0.3345), 373: tensor(0.3318), 494: tensor(0.3293), 612: tensor(0.3220), 37: tensor(0.3185), 651: tensor(0.3180), 748: tensor(0.3156), 622: tensor(0.3091), 472: tensor(0.3086), 422: tensor(0.3017), 305: tensor(0.2996), 664: tensor(0.2954), 430: tensor(0.2953), 628: tensor(0.2909), 583: tensor(0.2890), 810: tensor(0.2837), 597: tensor(0.2835), 775: tensor(0.2807), 287: tensor(0.2801), 283: tensor(0.2785), 826: tensor(0.2766), 306: tensor(0.2710), 469: tensor(0.2622), 712: tensor(0.2564), 507: tensor(0.2242), 590: tensor(0.2197), 479: tensor(0.2162), 668: tensor(0.2093), 294: tensor(0.1633)}\n"
     ]
    }
   ],
   "source": [
    "sorted_by_values_desc = dict(sorted(ranks.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_by_values_desc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "[1251, 1412, 1417, 1482, 494, 1775, 86, 247]\n",
      "False\n",
      "103\n",
      "[984, 2116, 255]\n",
      "False\n",
      "140\n",
      "[773, 775, 776, 1929, 1422, 1423, 1934, 1171, 1939, 790, 1562, 671, 800, 1183, 1186, 681, 42, 682, 941, 305, 948, 569, 1853, 1985, 322, 1603, 2243, 454, 1865, 1098, 1740, 978, 1879, 1752, 1755, 2268, 990, 738, 2021, 236, 1005, 1900, 108, 497, 1394, 2164, 1658]\n",
      "False\n",
      "111\n",
      "[1441, 2034, 423]\n",
      "False\n",
      "148\n",
      "[1668, 1798, 1288, 1033, 18, 2070, 1432, 1840, 1457, 568, 1594, 62, 64, 720, 1110, 471, 2135, 1507, 1891, 2284, 2288, 882, 887, 1149]\n",
      "False\n",
      "109\n",
      "[705, 450]\n",
      "False\n",
      "68\n",
      "[611, 37, 1034, 275, 1950]\n",
      "False\n",
      "142\n",
      "[2050, 4, 903, 1799, 1032, 1801, 1165, 528, 788, 1045, 536, 1050, 159, 1439, 1570, 421, 1789, 1838, 1790, 304, 2097, 690, 563, 949, 2102, 1081, 60, 317, 1220, 1093, 1480, 591, 594, 2010, 2140, 477, 479, 2016, 2027, 1388, 1402, 765, 126]\n",
      "False\n",
      "116\n",
      "[1106, 1238]\n",
      "False\n",
      "147\n",
      "[265, 1035, 271, 1809, 1300, 22, 284, 1692, 285, 1824, 425, 1963, 2219, 557, 45, 1210, 1727, 2240, 1728, 831, 2115, 2118, 1616, 1617, 97, 1896, 1770]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "mot=cumilating(tensors)\n",
    "count=0\n",
    "stuff=[]\n",
    "for j in sorted_by_values_desc:\n",
    "    stuff+=mot[j]   \n",
    "    count+=1\n",
    "    if(count==5):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n",
      "tensor(0.5538)\n",
      "[241]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "mot=cumilating(tensors)\n",
    "count=0\n",
    "for j in sorted_by_values_desc:\n",
    "    if(a[\"label\"].values[0] in mot[j]):\n",
    "        print(j)\n",
    "        print(sorted_by_values_desc[j])\n",
    "        print(mot[j])\n",
    "        print(a[\"label\"].values[0] in mot[j])\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modular orientations of random and quasi-random regular graphs'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['title'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec=df[df['label'].isin(stuff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>n_citation</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>references</th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>53e997ccb7602d9701fbd9c6</td>\n",
       "      <td>A local algorithm for finding dense subgraphs</td>\n",
       "      <td>2010</td>\n",
       "      <td>79</td>\n",
       "      <td>We present a local algorithm for finding dense...</td>\n",
       "      <td>[{'id': '53f4376fdabfaee0d9b6e367', 'name': 'R...</td>\n",
       "      <td>[53e9a131b7602d9702a232e3, 53e997ccb7602d9701f...</td>\n",
       "      <td>[-0.41132298110000004, 0.15002977850000002, -0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>53e99833b7602d97020587f0</td>\n",
       "      <td>Quasi-random graphs.</td>\n",
       "      <td>1989</td>\n",
       "      <td>483</td>\n",
       "      <td>We introduce a large equivalence class of grap...</td>\n",
       "      <td>[{'id': '5601bd4d45cedb3395eab3d4', 'name': 'F...</td>\n",
       "      <td>[53e9bd7bb7602d9704a1c870, 53e99833b7602d97020...</td>\n",
       "      <td>[-0.19190202650000002, 0.2339059711, -0.164615...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>53e9984bb7602d970207a0d3</td>\n",
       "      <td>Worst-case equilibria</td>\n",
       "      <td>2009</td>\n",
       "      <td>470</td>\n",
       "      <td>In a system where noncooperative agents share ...</td>\n",
       "      <td>[{'id': '54856a6bdabfaed7b5fa2146', 'name': 'E...</td>\n",
       "      <td>[53e997c6b7602d9701fb8204, 53e9a9a2b7602d97033...</td>\n",
       "      <td>[-0.2133031189, 0.3347230852, 0.00390023040000...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>53e998b0b7602d97020eca8f</td>\n",
       "      <td>The behavior of Wiener indices and polynomials...</td>\n",
       "      <td>2007</td>\n",
       "      <td>57</td>\n",
       "      <td>The sum of distances between all vertex pairs ...</td>\n",
       "      <td>[{'id': '53fa0527dabfae9467e15b40', 'name': 'W...</td>\n",
       "      <td>[53e99cb5b7602d970256854e, 53e9a4dcb7602d9702d...</td>\n",
       "      <td>[-0.5040803552, 0.0318988562, -0.1550520808000...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>53e998c8b7602d9702103b0d</td>\n",
       "      <td>beta -Perfect Graphs.</td>\n",
       "      <td>1996</td>\n",
       "      <td>53</td>\n",
       "      <td>The class of β -perfect graphs is introduced. ...</td>\n",
       "      <td>[{'id': '53f42eb8dabfaeb2acffb4c8', 'name': 'S...</td>\n",
       "      <td>[53e9ab89b7602d9703531c26]</td>\n",
       "      <td>[-0.4595018327, 0.2832813859, -0.1393359751, -...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71246</th>\n",
       "      <td>5c783b0c4895d9cbc6884f96</td>\n",
       "      <td>Algorithmic extensions of cheeger's inequality...</td>\n",
       "      <td>2011</td>\n",
       "      <td>27</td>\n",
       "      <td>We consider two generalizations of the problem...</td>\n",
       "      <td>[{'id': '53f43487dabfaedce551d4dd', 'name': 'A...</td>\n",
       "      <td>[53e9980eb7602d9702024fb5, 53e99991b7602d97021...</td>\n",
       "      <td>[-0.3984740973, 0.0642142519, -0.0710348785000...</td>\n",
       "      <td>2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71332</th>\n",
       "      <td>5c78df1c4895d9cbc6f736cd</td>\n",
       "      <td>On the family of r-regular graphs with Grundy ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>The Grundy number of a graph G, denoted by Γ(G...</td>\n",
       "      <td>[{'id': '53f43325dabfaedce550d7b2', 'name': 'N...</td>\n",
       "      <td>[53e99aa6b7602d9702320fd9, 53e99de8b7602d97026...</td>\n",
       "      <td>[-0.2726924419, -0.0486321077, -0.174098804600...</td>\n",
       "      <td>2243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73374</th>\n",
       "      <td>5ce2cebeced107d4c63021d7</td>\n",
       "      <td>On the geometric–arithmetic index of a graph</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>Very recently, Aouchiche and Hansen gave an up...</td>\n",
       "      <td>[{'id': '562cb25045cedb3398c9a066', 'name': 'Y...</td>\n",
       "      <td>[56ae9c290cf2a8c8f7148fca, 53e99aa6b7602d97023...</td>\n",
       "      <td>[-0.6259788275, -0.22742158170000001, 0.053956...</td>\n",
       "      <td>2268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73866</th>\n",
       "      <td>5d9edc4447c8f7664603988c</td>\n",
       "      <td>Approximation of biased Boolean functions of s...</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>The influence of the kth coordinate on a Boole...</td>\n",
       "      <td>[{'id': '53f4ce39dabfaeed20f8158a', 'name': 'N...</td>\n",
       "      <td>[53e9a102b7602d97029eea5e, 53e9a4f3b7602d9702e...</td>\n",
       "      <td>[-0.3337456882, 0.19411823150000002, -0.229343...</td>\n",
       "      <td>2284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74089</th>\n",
       "      <td>5e5a36bb93d709897c36914f</td>\n",
       "      <td>Vertex-isoperimetric stability in the hypercube.</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>Harper's Theorem states that, in a hypercube, ...</td>\n",
       "      <td>[{'id': '53f42ba9dabfaeb22f3edba7', 'name': 'M...</td>\n",
       "      <td>[53e99f70b7602d9702844928, 53e9a4f3b7602d9702e...</td>\n",
       "      <td>[-0.4139104187, 0.1805754453, -0.2703888118, -...</td>\n",
       "      <td>2288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "217    53e997ccb7602d9701fbd9c6   \n",
       "815    53e99833b7602d97020587f0   \n",
       "939    53e9984bb7602d970207a0d3   \n",
       "1415   53e998b0b7602d97020eca8f   \n",
       "1500   53e998c8b7602d9702103b0d   \n",
       "...                         ...   \n",
       "71246  5c783b0c4895d9cbc6884f96   \n",
       "71332  5c78df1c4895d9cbc6f736cd   \n",
       "73374  5ce2cebeced107d4c63021d7   \n",
       "73866  5d9edc4447c8f7664603988c   \n",
       "74089  5e5a36bb93d709897c36914f   \n",
       "\n",
       "                                                   title  year  n_citation  \\\n",
       "217        A local algorithm for finding dense subgraphs  2010          79   \n",
       "815                                 Quasi-random graphs.  1989         483   \n",
       "939                                Worst-case equilibria  2009         470   \n",
       "1415   The behavior of Wiener indices and polynomials...  2007          57   \n",
       "1500                               beta -Perfect Graphs.  1996          53   \n",
       "...                                                  ...   ...         ...   \n",
       "71246  Algorithmic extensions of cheeger's inequality...  2011          27   \n",
       "71332  On the family of r-regular graphs with Grundy ...  2013           6   \n",
       "73374       On the geometric–arithmetic index of a graph  2019           7   \n",
       "73866  Approximation of biased Boolean functions of s...  2017           1   \n",
       "74089   Vertex-isoperimetric stability in the hypercube.  2020           0   \n",
       "\n",
       "                                                abstract  \\\n",
       "217    We present a local algorithm for finding dense...   \n",
       "815    We introduce a large equivalence class of grap...   \n",
       "939    In a system where noncooperative agents share ...   \n",
       "1415   The sum of distances between all vertex pairs ...   \n",
       "1500   The class of β -perfect graphs is introduced. ...   \n",
       "...                                                  ...   \n",
       "71246  We consider two generalizations of the problem...   \n",
       "71332  The Grundy number of a graph G, denoted by Γ(G...   \n",
       "73374  Very recently, Aouchiche and Hansen gave an up...   \n",
       "73866  The influence of the kth coordinate on a Boole...   \n",
       "74089  Harper's Theorem states that, in a hypercube, ...   \n",
       "\n",
       "                                                 authors  \\\n",
       "217    [{'id': '53f4376fdabfaee0d9b6e367', 'name': 'R...   \n",
       "815    [{'id': '5601bd4d45cedb3395eab3d4', 'name': 'F...   \n",
       "939    [{'id': '54856a6bdabfaed7b5fa2146', 'name': 'E...   \n",
       "1415   [{'id': '53fa0527dabfae9467e15b40', 'name': 'W...   \n",
       "1500   [{'id': '53f42eb8dabfaeb2acffb4c8', 'name': 'S...   \n",
       "...                                                  ...   \n",
       "71246  [{'id': '53f43487dabfaedce551d4dd', 'name': 'A...   \n",
       "71332  [{'id': '53f43325dabfaedce550d7b2', 'name': 'N...   \n",
       "73374  [{'id': '562cb25045cedb3398c9a066', 'name': 'Y...   \n",
       "73866  [{'id': '53f4ce39dabfaeed20f8158a', 'name': 'N...   \n",
       "74089  [{'id': '53f42ba9dabfaeb22f3edba7', 'name': 'M...   \n",
       "\n",
       "                                              references  \\\n",
       "217    [53e9a131b7602d9702a232e3, 53e997ccb7602d9701f...   \n",
       "815    [53e9bd7bb7602d9704a1c870, 53e99833b7602d97020...   \n",
       "939    [53e997c6b7602d9701fb8204, 53e9a9a2b7602d97033...   \n",
       "1415   [53e99cb5b7602d970256854e, 53e9a4dcb7602d9702d...   \n",
       "1500                          [53e9ab89b7602d9703531c26]   \n",
       "...                                                  ...   \n",
       "71246  [53e9980eb7602d9702024fb5, 53e99991b7602d97021...   \n",
       "71332  [53e99aa6b7602d9702320fd9, 53e99de8b7602d97026...   \n",
       "73374  [56ae9c290cf2a8c8f7148fca, 53e99aa6b7602d97023...   \n",
       "73866  [53e9a102b7602d97029eea5e, 53e9a4f3b7602d9702e...   \n",
       "74089  [53e99f70b7602d9702844928, 53e9a4f3b7602d9702e...   \n",
       "\n",
       "                                                     emb  label  \n",
       "217    [-0.41132298110000004, 0.15002977850000002, -0...      4  \n",
       "815    [-0.19190202650000002, 0.2339059711, -0.164615...     18  \n",
       "939    [-0.2133031189, 0.3347230852, 0.00390023040000...     22  \n",
       "1415   [-0.5040803552, 0.0318988562, -0.1550520808000...     37  \n",
       "1500   [-0.4595018327, 0.2832813859, -0.1393359751, -...     42  \n",
       "...                                                  ...    ...  \n",
       "71246  [-0.3984740973, 0.0642142519, -0.0710348785000...   2240  \n",
       "71332  [-0.2726924419, -0.0486321077, -0.174098804600...   2243  \n",
       "73374  [-0.6259788275, -0.22742158170000001, 0.053956...   2268  \n",
       "73866  [-0.3337456882, 0.19411823150000002, -0.229343...   2284  \n",
       "74089  [-0.4139104187, 0.1805754453, -0.2703888118, -...   2288  \n",
       "\n",
       "[164 rows x 9 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
